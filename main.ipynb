{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42df0ccd",
   "metadata": {},
   "source": [
    "# Web Scraping using Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85012a0",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0026d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9751b81",
   "metadata": {},
   "source": [
    "### Part 1 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709fb20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Product Name\n",
    "def get_name(soup):\n",
    "\n",
    "    try:\n",
    "        title_string=soup.find(\"span\",attrs={\"id\":\"productTitle\"}).text.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "\n",
    "    return title_string\n",
    "\n",
    "# Function to extract Product Price\n",
    "def get_price(soup):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        price=soup.find(\"span\",attrs={\"class\":\"a-offscreen\"}).text\n",
    "\n",
    "    except AttributeError:\n",
    "\n",
    "        price = \"\"\n",
    "\n",
    "    return price\n",
    "\n",
    "# Function to extract Product Rating\n",
    "def get_rating(soup):\n",
    "\n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
    "    \n",
    "    except AttributeError:\n",
    "        try:\n",
    "            rating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "        except:\n",
    "            rating = \"\"\t\n",
    "\n",
    "    return rating\n",
    "\n",
    "# Function to extract Number of User Reviews\n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        review_count = \"\"\t\n",
    "\n",
    "    return review_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f91543",
   "metadata": {},
   "source": [
    "### Part 2 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba31d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract Product Description\n",
    "def get_product_description(soup):\n",
    "\n",
    "    try:\n",
    "        description=soup.find(\"div\",attrs={\"id\":\"feature-bullets\"}).text.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        description = \"\"\n",
    "\n",
    "    return description\n",
    "\n",
    "#Function to extract Manufacturer\n",
    "def get_manuf(soup):\n",
    "\n",
    "    try:\n",
    "        manuf=soup.find(\"div\",attrs={\"class\":\"a-section a-spacing-medium brand-snapshot-flex-row\"}).text.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        manuf= \"\"\n",
    "\n",
    "    return manuf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7debe7c1",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee42471",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The webpage URL\n",
    "    URL = \"https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1\"\n",
    "    \n",
    "    # Creating URL_list for 20 URL's of product pages\n",
    "    URL_List=[URL]\n",
    "    for i in range(19):\n",
    "        Pre_URL=URL_List[i]\n",
    "        webpage = requests.get(Pre_URL, headers=HEADERS)\n",
    "        print(webpage)\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "        URL2= 'https://www.amazon.in'+soup.find(\"a\", attrs={'class':\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"}).get('href')\n",
    "        URL_List.append(URL2)\n",
    "        \n",
    "    #Creating Dictionary    \n",
    "    d = {\"Product URL\":[],\"ASIN\":[],\"Manufacturer\":[], \"Product Name\":[],\"Product Description\":[], \"Product Price\":[], \"Rating\":[],\"Number of reviews\":[]}\n",
    "    \n",
    "    \n",
    "    #For each url \n",
    "    for urlx in URL_List:\n",
    "        print(urlx)\n",
    "        \n",
    "        # HTTP Request\n",
    "        webpage = requests.get(urlx, headers=HEADERS)\n",
    "        \n",
    "        # Soup Object containing all data\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "        \n",
    "        # Fetch links as List of Tag Objects\n",
    "        links = soup.find_all(\"a\", attrs={'class':'a-link-normal s-no-outline'})\n",
    "\n",
    "        # Store the links\n",
    "        links_list = []\n",
    "\n",
    "        # Loop for extracting links from Tag Objects\n",
    "        for link in links:\n",
    "            links_list.append(link.get('href'))\n",
    "\n",
    "    \n",
    "        # Loop for extracting product details from each link \n",
    "        for link in links_list:\n",
    "            try:\n",
    "                new_webpage = requests.get(\"https://www.amazon.in\" + link, headers=HEADERS)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "            \n",
    "            # Finding asin using Regular Expression \n",
    "            asin=re.search(r'[dg]p%2F([^%2F]+)',link, flags=re.IGNORECASE)\n",
    "            if(asin==None):\n",
    "                asin = re.search(r'[dg]p%2F([^%]+)', link, flags=re.IGNORECASE)\n",
    "            if(asin):\n",
    "                d[\"ASIN\"].append(asin.group(1))\n",
    "            else:\n",
    "                d[\"ASIN\"].append(\"\")\n",
    "            \n",
    "            # Finiding Product URL\n",
    "            d[\"Product URL\"].append(\"https://www.amazon.in\"+link)\n",
    "\n",
    "            # Function calls to display all necessary product information\n",
    "            d[\"Manufacturer\"].append(get_manuf(new_soup))\n",
    "            d[\"Product Description\"].append(get_product_description(new_soup))\n",
    "            d['Product Name'].append(get_name(new_soup))\n",
    "            d['Product Price'].append(get_price(new_soup))\n",
    "            d['Rating'].append(get_rating(new_soup))\n",
    "            d['Number of reviews'].append(get_review_count(new_soup))\n",
    "    \n",
    "    \n",
    "    amazon_df = pd.DataFrame.from_dict(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63da9f",
   "metadata": {},
   "source": [
    "### Checking the Extracted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b07180",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab50a245",
   "metadata": {},
   "source": [
    "### Exporting the Data to csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00deef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df.to_csv(\"amazon_data.csv\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
